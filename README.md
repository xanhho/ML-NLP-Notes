# ML-NLP-Notes

1. Attention Papers
- Neural Machine Translation by Jointly Learning to Align and Translate, https://arxiv.org/pdf/1409.0473.pdf
- Effective Approaches to Attention-based Neural Machine Translation, https://arxiv.org/pdf/1508.04025.pdf
- Sequence to Sequence Learning with Neural Networks, https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf

2. From Word2Vec to Pretrained Language Models Papers
- Semi-supervised Sequence Learning, https://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf
- Deep contextualized word representations, https://aclweb.org/anthology/N18-1202
- Attention is all you need, https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
- Universal Language Model Fine-tuning for Text Classification, https://aclweb.org/anthology/P18-1031
- Improving Language Understanding by Generative Pre-Training, https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/pdf/1810.04805.pdf
- Language Models are Unsupervised Multitask Learners, https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
